{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yIKQ-LR5nBk"
      },
      "source": [
        "## CS431/631 Data Intensive Distributed Analytics\n",
        "### Fall 2023 -- Personalized Page Rank\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdL9NkD55nBo"
      },
      "source": [
        "---\n",
        "#### Overview\n",
        "For this project, you will be using Python and Spark to perform some graph analysis, using a graph of the Gnutella server network.   In this graph, each node represents a server, and each (directed) edge represents a connection between servers in Gnutella's peer-to-peer network.  The input file for this assignment, `p2p-Gnutella08-adj.txt`, represents the graph as an adjacency list.   Each server (node) is identified by a unique number, and each line in the file gives the adjacency list for a single server.\n",
        "For example, this line:\n",
        "> 91\t243\t1923\t2194\n",
        "\n",
        "gives the adjacency list for server `91`.   It indicates that there are edges from server `91` to servers `243`, `1923`, and `2194`.    According to the Stanford Network Analysis Project, which collected these data, [the graph includes 6301 servers and 20777 edges](http://snap.stanford.edu/data/p2p-Gnutella08.html).\n",
        "\n",
        "Run the following block to install Spark and download the input file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PNw3CWW5nBp"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
        "!tar xzf spark-3.3.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/p2p-Gnutella08-adj.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnFWG115nBq"
      },
      "source": [
        "and then create a `SparkContext`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kx1nY435nBr"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2xfj_3e5nBt"
      },
      "source": [
        "---\n",
        "#### Question 1  (6/24 marks):\n",
        "\n",
        "To get warmed up, you should first write Spark code to confirm or determine some basic properties of the Gnutella graph.  Write code in the provided functions that will return answers to the following questions, as specified in each function's documentation:\n",
        "- How many nodes and edges are there in the graph?  (This should confirm the numbers given above.)\n",
        "- How many nodes of each outdegree are there? That is, how many nodes have no outgoing edges, how many have one outgoing edge, how many have two outgoing edges, and so on?\n",
        "- How many nodes of each indegree are there?\n",
        "\n",
        "You should use Spark to answer these questions.   Do *not* load the entire graph into your Python driver program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnoJMCQY5nBu"
      },
      "source": [
        "from operator import add\n",
        "\n",
        "def num_nodes_edges():\n",
        "    \"\"\"Returns a tuple (num_nodes, num_edges)\"\"\"\n",
        "    doc = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
        "    num_nodes = doc.count()\n",
        "    num_edges = doc.map(lambda line: line.split(\"\\t\")).\\\n",
        "        map(lambda x: len(x) -1).reduce(add)\n",
        "    return (num_nodes, num_edges)\n",
        "\n",
        "\n",
        "def out_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the outdegrees, and the\n",
        "    values are the number of nodes of the corresponding outdegree \"\"\"\n",
        "    #### Your code for Question 1.2 should go here\n",
        "    doc = sc.textFile(\"p2p-Gnutella08-adj.txt\").map(lambda line: line.split(\"\\t\"))\n",
        "    out_deg = doc.map(lambda x: ((len(x) -1),1)).reduceByKey(lambda x,y: x+y)\n",
        "    return out_deg.collectAsMap()\n",
        "\n",
        "\n",
        "\n",
        "def in_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the indegrees, and the\n",
        "    values are the number of nodes of the corresponding indegree \"\"\"\n",
        "    doc = sc.textFile(\"p2p-Gnutella08-adj.txt\").flatMap(lambda line: line.split(\"\\t\"))\n",
        "    # calculate the number of times each node shows up and reduce\n",
        "    in_deg = doc.map(lambda x: [x,1]).reduceByKey(lambda x,y: x+y).\\\n",
        "        map(lambda x: (x[1]-1,1)).reduceByKey(lambda x,y: x+y)\n",
        "    return in_deg.collectAsMap()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNp6bYDY5nBv"
      },
      "source": [
        "---\n",
        "Your main objective for this assignment is to perform *single source personalized page rank* over the Gnutella graph.  To get started, you should make sure that you have a clear understanding of ordinary (i.e., non-personalized) page rank.  Read the description of page rank in Section 5.3 of [the course textbook](https://lintool.github.io/MapReduceAlgorithms/index.html).   Personalized page rank is like ordinary page rank except:\n",
        "- One node in the graph is designated as the *source* node. Personalized page rank is performed with respect to that source node.\n",
        "- Personalized page rank is initialized by assigning all probability mass to the source node, and none to the other nodes. In contrast, ordinary page rank is initialized by giving all nodes the same probability mass.\n",
        "- Whenever personalized page rank makes a random jump, it jumps back to the source node. In contrast, ordinary page rank may jump to any node.\n",
        "- In personalized page rank, all probability mass lost dangling nodes is put back into the source nodes.  In ordinary page rank, lost mass is distributed evenly over all nodes.\n",
        "\n",
        "#### Question 2  (10/24 marks):\n",
        "\n",
        "Your task is to write a Spark program to perform personalized page rank over the Gnutella graph for a specified number of iterations, and of course a specific node. The function you will implement takes three input values:\n",
        "- source node id (a non-negative integer)\n",
        "- iteration count (a positive integer)\n",
        "- random jump factor value (a float between 0 and 1) - This is 1-B as introduced in the lecture.\n",
        "\n",
        "The function should perform personalized page rank, with respect to the specified source node, over the Gnutella graph, for the specified number of iterations, using Spark.\n",
        "The output of your function should be a list of the 10 nodes with the highest personalized page rank with respect to the given source. For each of the 10 nodes, return the node's id and page rank value as a tuple. The list returned by the function should therefore look something like this: `[(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oo6ny035nBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce61c7b-0990-4dc9-9c76-1e0ecd401bea"
      },
      "source": [
        "# helper function to compute the contribution * (1-beta) to its outneighbors\n",
        "#   or 0 for itself\n",
        "def contri(out_nbs, rank, beta):\n",
        "    for i in range(len(out_nbs)):\n",
        "      if i == 0:\n",
        "        yield [out_nbs[i],0]\n",
        "      else:\n",
        "        num_nbs = len(out_nbs)-1\n",
        "        yield [out_nbs[i], (1-beta)*rank/num_nbs]\n",
        "\n",
        "\n",
        "def personalized_page_rank(source_node_id, num_iterations, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "    # your solution to Question 2 here\n",
        "    doc = sc.textFile(\"p2p-Gnutella08-adj.txt\").map(lambda line: line.split(\"\\t\"))\n",
        "    # each list is in the form like\n",
        "    #     (node, [node, out-neighbor_1, ..., out-neighbor_k])\n",
        "    listdoc = doc.map(lambda x: (x[0], list(x)))\n",
        "    # initialize source node with mass 1 and other node with mass 0\n",
        "    ranklis = listdoc.map(lambda x:(x[0],1) if x[0] == str(source_node_id) else (x[0],0))\n",
        "    for iteration in range(num_iterations):\n",
        "      # (node, [list of out-neighbors, rank of node])\n",
        "      comb = listdoc.join(ranklis)\n",
        "      # distribute the mass\n",
        "      degs = comb.map(lambda x:x[1]).flatMap(lambda x: contri(x[0],x[1],jump_factor))\n",
        "      # mass of source node\n",
        "      loss = 1 - degs.map(lambda x: x[1]).sum()\n",
        "      # update rank\n",
        "      ranklis = degs.reduceByKey(lambda x,y:x+y).\\\n",
        "          map(lambda x: (x[0], loss) if x[0] == str(source_node_id) else x)\n",
        "    # sort by rank and take top 10\n",
        "    return ranklis.sortBy(lambda x: x[1], ascending = False).\\\n",
        "          map(lambda x: (int(x[0]), x[1])).take(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.3405274579137627),\n",
              " (9, 0.03250507933815258),\n",
              " (5, 0.03247639940230466),\n",
              " (7, 0.032362487667159025),\n",
              " (4, 0.0322172720112759),\n",
              " (3, 0.031500033458619965),\n",
              " (8, 0.031488430803692694),\n",
              " (10, 0.030872138035464868),\n",
              " (2, 0.030652673309598692),\n",
              " (6, 0.03065179010587444)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmZAajwV5nBw"
      },
      "source": [
        "---\n",
        "#### Question 3  (4/24 marks):\n",
        "\n",
        "For the previous question, you implemented personalized page rank that ran for a specified number of iterations.  However, it is also common to write iterative algorithms that run until some specified termination condition is reached.\n",
        "For example, for page rank, suppose the $p_i(x)$ represents the probability mass assigned to node $x$ after the $i$th iteration of the algorithm.  ($p_0(x)$ is the initial probability mass of node $x$.)   We define the change of $x$'s probability mass on the $i$th iteration as $\\lvert p_i(x)-p_{i-1}(x) \\rvert$.   Then, we can iterate personalized page rank until the maximum (over all nodes) change is less than a specified threshold, i.e, until all nodes' page ranks have converged.\n",
        "\n",
        "For this question, modify your personalized page rank implementation from Question 2 so that it iterates until the\n",
        "maximum node change is less than $\\frac{0.5}{N}$, where $N$ represents the number of nodes in the graph.\n",
        "This version of the function should take only two inputs: the source node id and the random jump factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whRf8BnZ5nBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f5ea5b-891b-47ec-cd10-f937f23516f9"
      },
      "source": [
        "def personalized_page_rank_stopping_criterion(source_node_id, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "    doc = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
        "    N = doc.count()\n",
        "    doc = doc.map(lambda line: line.split(\"\\t\"))\n",
        "    # each list is in the form like\n",
        "    #     (node, [node, out-neighbor_1, ..., out-neighbor_k])\n",
        "    listdoc = doc.map(lambda x: (x[0], list(x)))\n",
        "    listdoc = listdoc.repartition(listdoc.getNumPartitions()).cache()\n",
        "    # initialize source node with mass 1 and other node with mass 0\n",
        "    ranklis = listdoc.map(lambda x:(x[0],1) if x[0] == str(source_node_id) else (x[0],0))\n",
        "    change = 1\n",
        "    while change >= 0.5/N:\n",
        "      ranklis_prev = ranklis\n",
        "      # (node, [list of out-neighbors, rank of node])\n",
        "      comb = listdoc.join(ranklis)\n",
        "      # distribute the mass to out-neighbors\n",
        "      degs = comb.map(lambda x:x[1]).flatMap(lambda x: contri(x[0],x[1],jump_factor))\n",
        "      # mass of source node\n",
        "      loss = 1 - degs.map(lambda x: x[1]).sum()\n",
        "      # update rank\n",
        "      ranklis = degs.reduceByKey(lambda x,y:x+y).\\\n",
        "          map(lambda x: (x[0], loss) if x[0] == str(source_node_id) else x)\n",
        "      # calculate change (node, [prev_rank, rank_now])\n",
        "      change = ranklis.join(ranklis_prev).map(lambda x: abs(x[1][0]-x[1][1])).max()\n",
        "    _ = listdoc.unpersist()\n",
        "    # sort by rank and take top 10\n",
        "    return ranklis.sortBy(lambda x: x[1], ascending = False).\\\n",
        "        map(lambda x: (int(x[0]), x[1])).take(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.3405754456208271),\n",
              " (9, 0.032488188457622524),\n",
              " (5, 0.032462926597637755),\n",
              " (7, 0.032348695688724526),\n",
              " (4, 0.03220195432732894),\n",
              " (3, 0.03148505217375426),\n",
              " (8, 0.03147346845304872),\n",
              " (10, 0.030858384977842938),\n",
              " (2, 0.03063851843475855),\n",
              " (1, 0.030637624129176883)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}